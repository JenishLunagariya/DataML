{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Email: Spam/ not Spam?\n",
    "Online Transactions: Fraudulent(yes/no)?\n",
    "Tumor: Malignant/Benign?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y subset(£) of {0,1} ,   0: Negative class (benign tumor)\n",
    "                         1: positive class (malignant tumor)\n",
    "Only two class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Multiclass problem:\n",
    "y subset of {0,1,2,3}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hypothesis: h(x) = øT.x\n",
    "\n",
    "threshold classifier output h(x) at 0.5:\n",
    "    if h > 0.5 , predict y = 1 (malignent)\n",
    "    if h < 0.5 , predict y = 0 (not malignant)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If one data point is far away from the main cluster of data, then in Linear Regression, Model will include that\n",
    "point and give the resultant linear plot.\n",
    "But, logically we need to avoid that point, and include only clustered datasets, which will give us corrected \n",
    "model.\n",
    "hence, we should not use Linear regression in Classification Problems."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "h can be > 1 or < 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model:\n",
    "0 ≤ h(x) ≤ 1\n",
    "h(x) = g(øT.x)\n",
    "where, g(z) = 1/(1 + e^(-z))  called, Sigmoid function or Logistic function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hypothesis: h(x) = 1 / (1 + e^(øT.x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "h(x) = estimated probability that y = 1 on input x\n",
    "\n",
    "example: h = 0.7, there is 70 % probability of y being 1 (y = 1)(malignant)\n",
    "\n",
    "h(x) = P(y=1|x;ø) \"Probability that y=1, given x, parameterized by ø\"\n",
    "\n",
    "P(y=0) = 1 - P(y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Boundary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Suppose predict y=1 if h(x)≥0.5\n",
    "        predict y=0 if h(x)<0.5\n",
    "Now,\n",
    "In plot of Singmoid function, g(z) ≥ 0.5, when z ≥ 0\n",
    "hence, h(x) = g(øT.x) ≥ 0.5, when øT.x ≥ 0\n",
    "\n",
    "Likewise,\n",
    "g(øT.x)< 0.5, when øT.x < 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "h(x) = g(ø0 + ø1.x1 + ø2.x2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y≥1, when ø0 + ø1.x1 + ø2.x2 ≥0\n",
    "\n",
    "e.g. ø1 = -3 , ø1 = 1, ø2 = 1\n",
    "hence, z = -3 + x1 + x2 ≥ 0   ,for y≥1\n",
    "           x1 + x2 ≥ 3\n",
    "->This equation defines a line passing through (3,0) and (0,3)\n",
    "-> x1 + x2 ≥ 3, predicts y=1\n",
    "-> x1 + x2 < 3, predicts y=0\n",
    "\n",
    "->straight line is called Decision Boundary, and it is property of hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear Decision boundary "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "h(x) = g(ø0 + ø1.x1 + ø2.x2 + ø3.(x1^2) + ø4.(x2^2))\n",
    "suppose, ø0 = -1, ø1,ø2 = 0 , ø3,ø4 = 1\n",
    "\n",
    "z = -1 + x1^2 + x2^2 ≥ 0 \n",
    "    x1^2 + x2^2 ≥ 1       --> equation of circle with 1 radius and (0,0) as center"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "h(x) = g(ø0 + ø1.x1 + ø2.x2 + ø3.(x1^2) + ø4.(x1^2).x2 + ø5.(x1^2).(x2^2) + ø6.(x1^3).(x2) + ....)\n",
    "\n",
    "In this case, we will get more complex decision boundary than circle and line\n",
    "it will look like closed and irregular shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In case of Linear regression:\n",
    "J = (1/2m)∑(h(x)-y)^2\n",
    "can be rewrite as J = (1/m)∑(1/2)(h - y)^2\n",
    "                    = (1/m)∑ cost(h,y)\n",
    "\n",
    "cost(h(x),y) = (1/2)*(h(x)-y)^2    <-- non-convex function\n",
    "\n",
    "non-convex function: function with many local minimum, which fails convergence procedure\n",
    "convex function: function with only one and global minimum"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For Logistic Regression,\n",
    "\n",
    "J = (1/m)∑ Cost(h(x),y)\n",
    "\n",
    "\n",
    "                -log(h(x))   ,if y=1\n",
    "Cost(h(x),y) = \n",
    "                -log(1-h(x)) ,if y=0\n",
    "                \n",
    "                \n",
    "for y=1 case, cost = -log(h(x)), Now when x = 1, cost = 0 (minimim)\n",
    "                                          x = 0, cost = ∞\n",
    "                                          \n",
    "for y=0 case, cost = -log(1-h(x)), Now when x = 1, cost = ∞\n",
    "                                            x = 0, cost = 0 (minimum)        \n",
    "                                            \n",
    "Finally,\n",
    " Cost(h(x),y) = -ylog(h(x)) - (1-y)log(1-h(x))      <-- final cost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hence,\n",
    "\n",
    "J(ø) = -(1/m)∑[yi.log(h(xi)) + (1-yi).log(1-h(xi))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "find parameters ø, which will minimize the J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Repeat {\n",
    "      øj := øj - alpha∑(h(xi)-yi).xij\n",
    "}\n",
    "simultaneously update all ø"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Optimization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Optimization Algorithm:\n",
    "- Gradient descent\n",
    "- Conjugate gradient\n",
    "- BFGS\n",
    "- L-BFGS\n",
    "\n",
    "For Below 3 algorithm,\n",
    "Advantages:\n",
    "- No need to manually pick Alpha\n",
    "- Often faster than gradient descent\n",
    "\n",
    "Disadvantage:\n",
    "- More complex"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Example,\n",
    "ø = \n",
    "     ø1\n",
    "     ø2\n",
    "\n",
    "J(ø) = (ø1 - 5)^2 + (ø2 - 5)^2\n",
    "\n",
    "derivative of J(ø) w.r.t ø1 = 2(ø1 - 5)\n",
    "derivative of J(ø) w.r.t ø2 = 2(ø2 - 5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "theta = \n",
    "        ø0\n",
    "        ø1\n",
    "        ø2\n",
    "        .\n",
    "        .\n",
    "        øn\n",
    "\n",
    "function [jvals, gradient] = costFunction(theta)      <-- gredient is (n+1 x 1) matrix\n",
    "    jvals = [code to compute J(ø)]\n",
    "    gradient(1) = [code to compute derivative of J(ø) w.r.t ø0]\n",
    "    gradient(2) = [code to compute derivative of J(ø) w.r.t ø1]\n",
    "                                  .\n",
    "                                  .\n",
    "                                  .\n",
    "    gradient(n+1) = [code to compute derivative of J(ø) w.r.t øn]\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass classification "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Email foldering: work, friends, family    <-- (y=1,2,3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for binary classification, we get 2 clusters of data (y = 0 and y = 1)\n",
    "\n",
    "for Multiclass, we get 3(for email foldering example) clusters of data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SOne vs All(One-vs-rest):\n",
    "assume 3 classes:\n",
    " Class 1\n",
    " Class 2\n",
    " Class 3\n",
    "\n",
    "we will seperate 3 differnt binary classification on the training set\n",
    " First:  Class1 and (Class2 + Class3)\n",
    " Second: Class2 and (Class1 + Class3)\n",
    " Third:  Class3 and (Class1 + Class2)\n",
    " \n",
    "Then, when we need to predict for new input x, we run it on all 3 model, and choose which give max h(x), \n",
    "among 3 of them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
